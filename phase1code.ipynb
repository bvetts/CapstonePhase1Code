{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-shadow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-conservative",
   "metadata": {},
   "source": [
    "Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv('VLUE.csv')  \n",
    "value = pd.DataFrame()\n",
    "value[[\"date\"]] = val[[\"Date\"]]\n",
    "\n",
    "value[[\"adjClose\"]] = val[[\"Adj Close\"]]\n",
    "value = value.set_index(\"date\")\n",
    "value.loc[:,'yesterday'] = value.loc[:,'adjClose'].shift()#using difference from day to day\n",
    "\n",
    "\n",
    "value = value.dropna()\n",
    "#value[\"value\"] = value[\"adjClose\"]- value[\"yesterday\"]\n",
    "#change to percent difference\n",
    "\n",
    "value[\"value\"] = ((value[\"adjClose\"]- value[\"yesterday\"]) / ((value[\"adjClose\"]+ value[\"yesterday\"])/2)  ) *100\n",
    "\n",
    "value = value.drop(columns=['adjClose', 'yesterday'])\n",
    "\n",
    "siz = pd.read_csv('SIZE.csv')  \n",
    "size = pd.DataFrame()\n",
    "size[[\"date\"]] = siz[[\"Date\"]]\n",
    "size[[\"adjClose\"]] = siz[[\"Adj Close\"]]\n",
    "\n",
    "size = size.set_index(\"date\")\n",
    "size.loc[:,'yesterday'] = size.loc[:,'adjClose'].shift()\n",
    "size = size.dropna()\n",
    "#size[\"size\"] = size[\"adjClose\"]- size[\"yesterday\"]\n",
    "\n",
    "size[\"size\"] = ((size[\"adjClose\"]- size[\"yesterday\"]) / ((size[\"adjClose\"]+ size[\"yesterday\"])/2)  ) *100\n",
    "\n",
    "\n",
    "\n",
    "size = size.drop(columns=['adjClose', 'yesterday'])\n",
    "#do a percentagr change not a numerical change \n",
    "\n",
    "m = pd.read_csv('MTUM.csv')  \n",
    "mtum = pd.DataFrame()\n",
    "mtum[[\"date\"]] = m[[\"Date\"]]\n",
    "\n",
    "mtum[[\"adjClose\"]] = m[[\"Adj Close\"]]\n",
    "\n",
    "mtum = mtum.set_index(\"date\")\n",
    "mtum.loc[:,'yesterday'] = mtum.loc[:,'adjClose'].shift()\n",
    "mtum = mtum.dropna()\n",
    "#mtum[\"mtum\"] = mtum[\"adjClose\"]- mtum[\"yesterday\"]\n",
    "\n",
    "mtum[\"mtum\"] = ((mtum[\"adjClose\"]- mtum[\"yesterday\"]) / ((mtum[\"adjClose\"]+ mtum[\"yesterday\"])/2)  ) *100\n",
    "\n",
    "\n",
    "mtum = mtum.drop(columns=['adjClose', 'yesterday'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-greensboro",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = mtum.join(value)\n",
    "factors = factors.join(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pd.read_csv('GOOG.csv')  \n",
    "google = pd.DataFrame()\n",
    "google[[\"date\"]] = g[[\"Date\"]]\n",
    "google[[\"adjClose\"]] = g[[\"Adj Close\"]]\n",
    "\n",
    "google = google.set_index(\"date\")\n",
    "\n",
    "google.loc[:,'yesterday'] = google.loc[:,'adjClose'].shift()\n",
    "google = google.dropna()\n",
    "#google[\"google\"] = google[\"adjClose\"]- google[\"yesterday\"]\n",
    "\n",
    "google[\"google\"] = ((google[\"adjClose\"]- google[\"yesterday\"]) / ((google[\"adjClose\"]+ google[\"yesterday\"])/2)  ) *100\n",
    "\n",
    "\n",
    "google = google.drop(columns=['adjClose', 'yesterday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pd.read_csv('WFC.csv')  \n",
    "wfc = pd.DataFrame()\n",
    "wfc[[\"date\"]] = g[[\"Date\"]]\n",
    "wfc[[\"adjClose\"]] = g[[\"Adj Close\"]]\n",
    "\n",
    "wfc = wfc.set_index(\"date\")\n",
    "\n",
    "wfc.loc[:,'yesterday'] = wfc.loc[:,'adjClose'].shift()\n",
    "wfc = wfc.dropna()\n",
    "#google[\"google\"] = google[\"adjClose\"]- google[\"yesterday\"]\n",
    "\n",
    "wfc[\"wfc\"] = ((wfc[\"adjClose\"]- wfc[\"yesterday\"]) / ((wfc[\"adjClose\"]+ wfc[\"yesterday\"])/2)  ) *100\n",
    "\n",
    "\n",
    "wfc = wfc.drop(columns=['adjClose', 'yesterday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-appearance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making bigger factor dataset\n",
    "\n",
    "b = pd.read_csv('BTC-USD.csv')  \n",
    "bitcoin = pd.DataFrame()\n",
    "bitcoin[[\"date\"]] = b[[\"Date\"]]\n",
    "bitcoin[[\"adjClose\"]] = b[[\"Adj Close\"]]\n",
    "bitcoin = bitcoin.set_index(\"date\")\n",
    "bitcoin.loc[:,'yesterday'] = bitcoin.loc[:,'adjClose'].shift()\n",
    "bitcoin = bitcoin.dropna()\n",
    "#bitcoin[\"bitcoin\"] = bitcoin[\"adjClose\"]- bitcoin[\"yesterday\"]\n",
    "\n",
    "bitcoin[\"bitcoin\"] = ((bitcoin[\"adjClose\"]- bitcoin[\"yesterday\"]) / ((bitcoin[\"adjClose\"]+ bitcoin[\"yesterday\"])/2)  ) *100\n",
    "\n",
    "\n",
    "\n",
    "bitcoin =bitcoin.drop(columns=['adjClose', 'yesterday'])\n",
    "\n",
    "\n",
    "c = pd.read_csv('CMA.csv')  \n",
    "cma = pd.DataFrame()\n",
    "cma[[\"date\"]] = c[[\"Date\"]]\n",
    "cma[[\"adjClose\"]] = c[[\"Adj Close\"]]\n",
    "\n",
    "cma = cma.set_index(\"date\")\n",
    "cma.loc[:,'yesterday'] = cma.loc[:,'adjClose'].shift()\n",
    "cma = cma.dropna()\n",
    "#cma[\"cma\"] = cma[\"adjClose\"]- cma[\"yesterday\"]\n",
    "cma[\"cma\"] = ((cma[\"adjClose\"]- cma[\"yesterday\"]) / ((cma[\"adjClose\"] + cma[\"yesterday\"])/2)  ) *100\n",
    "\n",
    "\n",
    "cma = cma.drop(columns=['adjClose', 'yesterday'])\n",
    "\n",
    "\n",
    "\n",
    "r = pd.read_csv('RWM.csv')  \n",
    "rwm = pd.DataFrame()\n",
    "rwm[[\"date\"]] = r[[\"Date\"]]\n",
    "rwm[[\"adjClose\"]] = r[[\"Adj Close\"]]\n",
    "\n",
    "rwm = rwm.set_index(\"date\")\n",
    "rwm.loc[:,'yesterday'] = rwm.loc[:,'adjClose'].shift()\n",
    "rwm = rwm.dropna()\n",
    "#rwm[\"rwm\"] = rwm[\"adjClose\"]- rwm[\"yesterday\"]\n",
    "\n",
    "rwm[\"rwm\"] = ((rwm[\"adjClose\"]- rwm[\"yesterday\"]) / ((rwm[\"adjClose\"] + rwm[\"yesterday\"])/2)  ) *100\n",
    "\n",
    "\n",
    "rwm = rwm.drop(columns=['adjClose', 'yesterday'])\n",
    "\n",
    "\n",
    "v = pd.read_csv('VIX.csv')  \n",
    "vix = pd.DataFrame()\n",
    "vix[[\"date\"]] = v[[\"Date\"]]\n",
    "vix[[\"adjClose\"]] = v[[\"Adj Close\"]]\n",
    "\n",
    "vix = vix.set_index(\"date\")\n",
    "vix.loc[:,'yesterday'] = vix.loc[:,'adjClose'].shift()\n",
    "vix = vix.dropna()\n",
    "#vix[\"vix\"] = vix[\"adjClose\"]- vix[\"yesterday\"]\n",
    "vix[\"vix\"] = ((vix[\"adjClose\"]- vix[\"yesterday\"]) / ((vix[\"adjClose\"] + vix[\"yesterday\"])/2)  ) *100\n",
    "\n",
    "\n",
    "vix = vix.drop(columns=['adjClose', 'yesterday'])\n",
    "\n",
    "\n",
    "\n",
    "r2 = pd.read_csv('RINF.csv')  \n",
    "rinf = pd.DataFrame()\n",
    "rinf[[\"date\"]] = r2[[\"Date\"]]\n",
    "rinf[[\"adjClose\"]] = r2[[\"Adj Close\"]]\n",
    "\n",
    "rinf = rinf.set_index(\"date\")\n",
    "rinf.loc[:,'yesterday'] = rinf.loc[:,'adjClose'].shift()\n",
    "rinf = rinf.dropna()\n",
    "#rinf[\"rinf\"] = rinf[\"adjClose\"]- rinf[\"yesterday\"]\n",
    "rinf[\"rinf\"] = ((rinf[\"adjClose\"]- rinf[\"yesterday\"]) / ((rinf[\"adjClose\"] + rinf[\"yesterday\"])/2)  ) *100\n",
    "\n",
    "\n",
    "rinf = rinf.drop(columns=['adjClose', 'yesterday'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "factorsBig = pd.DataFrame()\n",
    "factorsBig = factors\n",
    "factorsBig = factorsBig.join(bitcoin)\n",
    "\n",
    "factorsBig = factorsBig.join(cma)\n",
    "factorsBig = factorsBig.join(rwm)\n",
    "factorsBig = factorsBig.join(vix)\n",
    "factorsBig = factorsBig.join(rinf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "factorsScaled = factors.copy()\n",
    "\n",
    "for column in factorsScaled.columns:\n",
    "    factorsScaled[column] = (factorsScaled[column] - factorsScaled[column].mean()) / (factorsScaled[column].std())    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-ferry",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pandas.pydata.org/pandas-docs/version/0.13.1/visualization.html\n",
    "plt.figure(figsize=(16, 8), dpi=150)\n",
    "\n",
    "factorsScaled['mtum'].plot(label='mtum')\n",
    "factorsScaled['value'].plot(label='value')\n",
    "factorsScaled['size'].plot(label='size')\n",
    "\n",
    "plt.title('3 factor model')\n",
    "plt.xlabel('date')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = factors.join(google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = factors.join(google)\n",
    "dataset = dataset.join(wfc)\n",
    "for column in dataset.columns:\n",
    "    dataset[column] = (dataset[column] - dataset[column].mean()) / (dataset[column].std())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-length",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8), dpi=150)\n",
    "\n",
    "dataset['mtum'].plot(label='mtum')\n",
    "dataset['value'].plot(label='value')\n",
    "dataset['size'].plot(label='size')\n",
    "dataset['google'].plot(label='google')\n",
    "dataset['wfc'].plot(label='wfc')\n",
    "\n",
    "plt.title('3 factor model')\n",
    "plt.xlabel('date')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetBig = factorsBig.join(google)\n",
    "datasetBig = datasetBig.join(wfc)\n",
    "for column in datasetBig.columns:\n",
    "    datasetBig[column] = (datasetBig[column] - datasetBig[column].mean()) / (datasetBig[column].std())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8), dpi=150)\n",
    "  \n",
    "\n",
    "datasetBig['mtum'].plot(label='mtum')\n",
    "datasetBig['value'].plot(label='value')\n",
    "datasetBig['size'].plot(label='size')\n",
    "datasetBig['bitcoin'].plot(label='bitcoin')\n",
    "datasetBig['cma'].plot(label='cma')\n",
    "datasetBig['rwm'].plot(label='rwm')\n",
    "datasetBig['vix'].plot(label='vix')\n",
    "datasetBig['rinf'].plot(label='rinf')\n",
    "datasetBig['google'].plot(label='google')\n",
    "datasetBig['wfc'].plot(label='wfc')\n",
    "\n",
    "plt.title('All Currently included factors')\n",
    "plt.xlabel('date')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetBig.hist(figsize = (12,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "C_mat = datasetBig.corr()\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "\n",
    "sns.heatmap(C_mat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-broad",
   "metadata": {},
   "source": [
    "Low Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "#google version \n",
    "dataset_scaled = dataset.copy()\n",
    "dataset_scaled = dataset_scaled.drop(['wfc'], axis = 1)\n",
    "train = dataset_scaled .iloc[:450,:]\n",
    "test = dataset_scaled .iloc[451:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['google'], axis = 1)\n",
    "y_train = train[[\"google\"]]\n",
    "\n",
    "X_test = test.drop(['google'], axis = 1)\n",
    "y_test = test[[\"google\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "train = reg.score(X_train, y_train)\n",
    "test = reg.score(X_test, y_test)#this is R squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train R2:\", train)\n",
    "print(\"test R2:\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wfc version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scaled = dataset.copy()\n",
    "dataset_scaled = dataset_scaled.drop(['google'], axis = 1)\n",
    "\n",
    "train = dataset_scaled .iloc[:450,:]\n",
    "test = dataset_scaled .iloc[451:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['wfc'], axis = 1)\n",
    "y_train = train[[\"wfc\"]]\n",
    "\n",
    "X_test = test.drop(['wfc'], axis = 1)\n",
    "y_test = test[[\"wfc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "train = reg.score(X_train, y_train)\n",
    "test = reg.score(X_test, y_test)#this is R squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train R2:\", train)\n",
    "print(\"test R2:\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-possible",
   "metadata": {},
   "source": [
    "Medium Risk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#big dataset google \n",
    "dataset_scaledBig = datasetBig.copy()\n",
    "dataset_scaledBig = dataset_scaledBig.drop(['wfc'], axis = 1)\n",
    "train = dataset_scaledBig.iloc[:450,:]\n",
    "test = dataset_scaledBig.iloc[451:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['google'], axis = 1)\n",
    "y_train = train[[\"google\"]]\n",
    "\n",
    "X_test = test.drop(['google'], axis = 1)\n",
    "y_test = test[[\"google\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "train = reg.score(X_train, y_train)\n",
    "test = reg.score(X_test, y_test)#this is R squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train R2:\", train)\n",
    "print(\"test R2:\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#still big dataset with google\n",
    "reg = Lasso(alpha=0.00003).fit(X_train, y_train)\n",
    "train = reg.score(X_train, y_train)\n",
    "test = reg.score(X_test, y_test)#this is R squared, coefficient of determination\n",
    "\n",
    "print(\"train R2:\", train)\n",
    "print(\"test R2:\", test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Lasso(alpha=0.00000000003).fit(X_train, y_train)\n",
    "train = reg.score(X_train, y_train)\n",
    "test = reg.score(X_test, y_test)#this is R squared\n",
    "print(\"train R2:\", train)\n",
    "print(\"test R2:\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "#big dataset wells fargo \n",
    "\n",
    "dataset_scaledBig = datasetBig.copy()\n",
    "dataset_scaledBig = dataset_scaledBig.drop(['google'], axis = 1)\n",
    "train = dataset_scaledBig.iloc[:450,:]\n",
    "test = dataset_scaledBig.iloc[451:,:]\n",
    "\n",
    "X_train = train.drop(['wfc'], axis = 1)\n",
    "y_train = train[[\"wfc\"]]\n",
    "\n",
    "X_test = test.drop(['wfc'], axis = 1)\n",
    "y_test = test[[\"wfc\"]]\n",
    "\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "train = reg.score(X_train, y_train)\n",
    "test = reg.score(X_test, y_test)#this is R squared\n",
    "\n",
    "print(\"train R2:\", train)\n",
    "print(\"test R2:\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wells sparse version \n",
    "reg = Lasso(alpha=0.0237).fit(X_train, y_train)\n",
    "train = reg.score(X_train, y_train)\n",
    "test = reg.score(X_test, y_test)#this is R squared\n",
    "\n",
    "print(\"train R2:\", train)\n",
    "print(\"test R2:\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-nicholas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "confused-facility",
   "metadata": {},
   "source": [
    "High Risk "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-seafood",
   "metadata": {},
   "source": [
    "Wells fargo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ContextualizedRegressor import ContextualizedRegressor\n",
    "import dateutil\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-piece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scaledBig = datasetBig.copy()\n",
    "\n",
    "#only one of the following \n",
    "dataset_scaledBig = dataset_scaledBig.drop(['google'], axis = 1)#for wfc results\n",
    "#dataset_scaledBig = dataset_scaledBig.drop(['wfc'], axis = 1)#for google results\n",
    "\n",
    "train = dataset_scaledBig.iloc[:450,:]\n",
    "test = dataset_scaledBig.iloc[451:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-angel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For wfc results\n",
    "X_train = train.drop(['wfc'], axis = 1)\n",
    "y_train = train[[\"wfc\"]]\n",
    "\n",
    "X_test = test.drop(['wfc'], axis = 1)\n",
    "y_test = test[[\"wfc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcopy2 = X_train\n",
    "Xcopy3 = X_test\n",
    "Xcopy2.reset_index(level=0, inplace=True)\n",
    "Xcopy3.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_intx = [dateutil.parser.parse(x) for x in list(Xcopy2['date'])]\n",
    "sorted_dates = sorted(dates_intx)\n",
    "sorted_dates = [x for x in sorted_dates ]\n",
    "sorted_dates_str = [str(x.date()) for x in sorted_dates]\n",
    "context = [c - sorted_dates[0] for c in sorted_dates[0:]]\n",
    "context = np.expand_dims(np.array([c.days for c in context]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_intx2 = [dateutil.parser.parse(x) for x in list(Xcopy3['date'])]\n",
    "sorted_dates2 = sorted(dates_intx2)\n",
    "sorted_dates2 = [x for x in sorted_dates2 ]\n",
    "sorted_dates_str2 = [str(x.date()) for x in sorted_dates2]\n",
    "context2 = [c - sorted_dates2[0] for c in sorted_dates2[0:]]\n",
    "context2 = np.expand_dims(np.array([c.days for c in context2]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "class input_transformer:\n",
    "    def __init__(self, mean=0, std=1, append_ones=False):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.append_ones = append_ones\n",
    "        \n",
    "    def transform(self, raw):\n",
    "        transformed = (raw - self.mean) / self.std\n",
    "        if self.append_ones:\n",
    "            return np.hstack((transformed, np.ones((len(transformed), 1))))\n",
    "        else:\n",
    "            return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_transformer = input_transformer(np.mean(context),\n",
    "                                           np.std(context),\n",
    "                                           append_ones=True)\n",
    "context_transformed = context_transformer.transform(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_transformer = input_transformer(np.mean(context2),\n",
    "                                           np.std(context2),\n",
    "                                           append_ones=True)\n",
    "context_transformed2 = context_transformer.transform(context2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_returns = X_train[['mtum','value', 'size' ]].values  #select columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y=y_train['google'].values\n",
    "y=y_train['wfc'].values\n",
    "asset_returns  = np.expand_dims(y, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context_transformed.shape, factor_returns.shape, asset_returns.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = ContextualizedRegressor(\n",
    "        context_shape=context_transformed.shape[1:],\n",
    "        data_shape=factor_returns.shape[1:],\n",
    "        output_shape=asset_returns.shape[1:],\n",
    "        n_archetypes=16,\n",
    "        sample_specific_loss_params=None,\n",
    "        archetype_loss_params={'regularizer': tf.keras.regularizers.l2(1e0)},\n",
    "        n_encoder_layers=5, encoder_width=32,\n",
    "        context_activity_regularizer=tf.keras.regularizers.l1(0.),\n",
    "        activation='sigmoid',\n",
    "        init_archs=None, freeze_archs=False,\n",
    "        learning_rate=1e-4,\n",
    "        tf_dtype=tf.dtypes.float32,\n",
    "        pop_model=None,\n",
    "        base_predictor=None,\n",
    "        encoder_type='dnn',\n",
    "        loss=\"mse\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model.fit(C=context_transformed,\n",
    "                 X=factor_returns, Y=asset_returns,\n",
    "                 epochs=100, batch_size=1,\n",
    "                 es_patience=3, val_split=0.25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-selection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_small = small_model.predict(context_transformed,factor_returns)\n",
    "\n",
    "r2_score(asset_returns, y_pred_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for few factors\n",
    "x_t=X_test[['mtum','value', 'size' ]].values\n",
    "x_t = np.expand_dims(x_t, axis = 1) \n",
    "x_t =np.squeeze(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-journalist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_t=y_test['google'].values\n",
    "y_t_small=y_test['wfc'].values\n",
    "y_t_small = np.expand_dims(y_t_small, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_t = small_model.predict(context_transformed2, x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t_small = small_model.predict(context_transformed2, x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score( y_t_small, y_pred_t_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( y_t_small, y_pred_t_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-eligibility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-problem",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-rough",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "#array([-0.        ,  0.45816352,  0.        ,  0.        ,  0.43456671,\n",
    "#       -0.        ,  0.00226671, -0.00851   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_returns = X_train[['mtum','value', 'size', 'bitcoin', 'cma', 'rwm', 'vix','rinf' ]].values \n",
    "#factor_returns = X_train[['value',  'cma', 'vix','rinf' ]].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y=y_train['google'].values\n",
    "y=y_train['wfc'].values\n",
    "asset_returns  = np.expand_dims(y, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_model = ContextualizedRegressor(\n",
    "        context_shape=context_transformed.shape[1:],\n",
    "        data_shape=factor_returns.shape[1:],\n",
    "        output_shape=asset_returns.shape[1:],\n",
    "        n_archetypes=16,\n",
    "        sample_specific_loss_params=None,\n",
    "        archetype_loss_params={'regularizer': tf.keras.regularizers.l2(1e0)},\n",
    "        n_encoder_layers=5, encoder_width=32,\n",
    "        context_activity_regularizer=tf.keras.regularizers.l1(0.),\n",
    "        activation='sigmoid',\n",
    "        init_archs=None, freeze_archs=False,\n",
    "        learning_rate=1e-4,\n",
    "        tf_dtype=tf.dtypes.float32,\n",
    "        pop_model=None,\n",
    "        base_predictor=None,\n",
    "        encoder_type='dnn',\n",
    "        loss=\"mse\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_model.fit(C=context_transformed,\n",
    "                 X=factor_returns, Y=asset_returns,\n",
    "                 epochs=100, batch_size=1,\n",
    "                 es_patience=3, val_split=0.25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_big = big_model.predict(context_transformed,factor_returns)\n",
    "\n",
    "r2_score(asset_returns, y_pred_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for big factors\n",
    "x_t=X_test[['mtum','value', 'size', 'bitcoin', 'cma', 'rwm', 'vix','rinf' ]].values  #select columns #\n",
    "#x_t=X_test[['value',  'cma', 'vix','rinf' ]].values\n",
    "\n",
    "x_t = np.expand_dims(x_t, axis = 1) \n",
    "x_t =np.squeeze(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_t=y_test['google'].values\n",
    "y_t_big=y_test['wfc'].values\n",
    "y_t_big = np.expand_dims(y_t_big, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t_big = big_model.predict(context_transformed2, x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score( y_t_big, y_pred_t_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( y_t_big, y_pred_t_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do this for the predicted section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_coefficents = big_model.predict_coefs(context_transformed2)\n",
    "y_coefficents_small = small_model.predict_coefs(context_transformed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_coefficents_small.shape #row column , 82 inscances for the 8 factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-alexandria",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_coefficents_small.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is all for 3 factor at the moment \n",
    "#get with and without unexplained???\n",
    "\n",
    "#maybe lot the predicted and actual results too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-briefing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need the unexplaned information \n",
    "\n",
    "factors = ['mtum','value', 'size' ]\n",
    "for j, factor in enumerate(factors):\n",
    "    unexplained = (np.squeeze(y_t_small) - np.squeeze(y_pred_t_small)) / np.std(y_t_small)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible plot: unfinished. do this instead of using unexplained with coefficents??\n",
    "handles = []\n",
    "fig, ax = plt.subplots(3 ,1,sharex=True, sharey=True, figsize=(20, 3*len(factors)))\n",
    "handles.append(ax[0].bar(Xcopy3['date'], np.squeeze(y_pred_t_small), label='unexplaned' , color='red') )  #predicted\n",
    "handles.append(ax[1].bar(Xcopy3['date'], np.squeeze(y_t_small), label='unexplaned' , color='blue') )  #actual\n",
    "handles.append(ax[2].bar(Xcopy3['date'], unexplained.squeeze(), label='unexplaned' , color='slategrey') ) #unexplaned\n",
    "plt.xticks(np.arange(0, len(Xcopy3['date']) + 1, 20))\n",
    "\n",
    "fig.suptitle(\"Wells Fargo 3 Factor Predicted, Actual, Unexplained\", fontsize=30)\n",
    "labels = ['Predicted', 'Actual', 'Unexplained']\n",
    "plt.xlabel(\"Date\" , fontsize=20)\n",
    "lgd = plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-example",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-space",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#its ploting strength of the factors, so postive and less then i 1 makes sense maybe?? missing something>>\n",
    "#incomplete results???\n",
    "factors = ['mtum','value', 'size' ]\n",
    "handles = []\n",
    "plot_offset=2\n",
    "colors=['red','blue', 'orange', 'black', 'purple', 'green', 'tan', 'cyan']\n",
    "\n",
    "fig, ax = plt.subplots(len(factors)+1 ,1,sharex=True, sharey=True, figsize=(20, 3*len(factors)))\n",
    "\n",
    "for j, factor in enumerate(factors):\n",
    "    handles.append(ax[j].bar(Xcopy3['date'], y_coefficents_small[:, j].squeeze(), label=factor , color=colors[j]) )\n",
    "    ax[j].set_ylabel(\"Factor Weight\")\n",
    "    plt.xticks(np.arange(0, len(Xcopy3['date']) + 1, 20))\n",
    "\n",
    "handles.append(ax[-1].bar(Xcopy3['date'], unexplained.squeeze(), label='unexplaned' , color='slategrey') )   \n",
    "ax[-1].set_ylabel(\"Factor Weight\")\n",
    "    \n",
    "labels = [f for f in factors] +['unexplained']\n",
    "fig.suptitle(\"Wells Fargo 3 Factor Weights with Unexplained\", fontsize=30)\n",
    "\n",
    "plt.xlabel(\"Date\" , fontsize=20)\n",
    "lgd = plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-contact",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = ['mtum','value', 'size' ]\n",
    "handles = []\n",
    "plot_offset=2\n",
    "colors=['red','blue', 'orange', 'black', 'purple', 'green', 'tan', 'cyan']\n",
    "\n",
    "fig, ax = plt.subplots(len(factors) ,1,sharex=True, sharey=True, figsize=(20, 3*len(factors)))\n",
    "\n",
    "for j, factor in enumerate(factors):\n",
    "    handles.append(ax[j].bar(Xcopy3['date'], y_coefficents_small[:, j].squeeze(), label=factor , color=colors[j]) )\n",
    "    ax[j].set_ylabel(\"Factor Weight\")\n",
    "    plt.xticks(np.arange(0, len(Xcopy3['date']) + 1, 20))\n",
    "\n",
    "\n",
    "    \n",
    "labels = [f for f in factors] \n",
    "fig.suptitle(\"Wells Fargo 3 Factor Weights\", fontsize=30)\n",
    "\n",
    "plt.xlabel(\"Date\" , fontsize=20)\n",
    "lgd = plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_coefficents_big = big_model.predict_coefs(context_transformed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-inquiry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-thong",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "factors = ['mtum','value', 'size', 'bitcoin', 'cma', 'rwm', 'vix','rinf' ]\n",
    "for j, factor in enumerate(factors):\n",
    "    unexplained = (np.squeeze(y_t_big) - np.squeeze(y_pred_t_big)) / np.std(y_t_big)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible plot: unfinished. do this instead of using unexplained with coefficents??\n",
    "handles = []\n",
    "fig, ax = plt.subplots(3 ,1,sharex=True, sharey=True, figsize=(20, 3*3))\n",
    "handles.append(ax[0].bar(Xcopy3['date'], np.squeeze(y_pred_t_big), label='unexplaned' , color='red') )  #predicted\n",
    "handles.append(ax[1].bar(Xcopy3['date'], np.squeeze(y_t_big), label='unexplaned' , color='blue') )  #actual\n",
    "handles.append(ax[2].bar(Xcopy3['date'], unexplained.squeeze(), label='unexplaned' , color='slategrey') ) #unexplaned\n",
    "plt.xticks(np.arange(0, len(Xcopy3['date']) + 1, 20))\n",
    "\n",
    "fig.suptitle(\"Wells Fargo 8 Factor Predicted, Actual, Unexplained\", fontsize=30)\n",
    "labels = ['Predicted', 'Actual', 'Unexplained']\n",
    "plt.xlabel(\"Date\" , fontsize=20)\n",
    "lgd = plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#its ploting strength of the factors, so postive and less then i 1 makes sense maybe?? missing something>>\n",
    "#incomplete results???\n",
    "factors = ['mtum','value', 'size', 'bitcoin', 'cma', 'rwm', 'vix','rinf' ]\n",
    "handles = []\n",
    "plot_offset=2\n",
    "colors=['red','blue', 'orange', 'black', 'purple', 'green', 'tan', 'skyblue']\n",
    "\n",
    "fig, ax = plt.subplots(len(factors) ,1,sharex=True, sharey=True, figsize=(20, 3*len(factors)))\n",
    "\n",
    "for j, factor in enumerate(factors):\n",
    "    handles.append(ax[j].bar(Xcopy3['date'], y_coefficents_big[:, j].squeeze(), label=factor , color=colors[j]) )\n",
    "    ax[j].set_ylabel(\"Factor Weight\")\n",
    "    plt.xticks(np.arange(0, len(Xcopy3['date']) + 1, 20))\n",
    "\n",
    "\n",
    "labels = [f for f in factors] +['unexplained']\n",
    "fig.suptitle(\"Wells Fargo 8 Factor Weights with Unexplained\", fontsize=30)\n",
    "\n",
    "plt.xlabel(\"Date\" , fontsize=20)\n",
    "lgd = plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-hotel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#its ploting strength of the factors, so postive and less then i 1 makes sense maybe?? missing something>>\n",
    "#incomplete results???\n",
    "factors = ['mtum','value', 'size', 'bitcoin', 'cma', 'rwm', 'vix','rinf' ]\n",
    "handles = []\n",
    "plot_offset=2\n",
    "colors=['red','blue', 'orange', 'black', 'purple', 'green', 'tan', 'skyblue']\n",
    "\n",
    "fig, ax = plt.subplots(len(factors) ,1,sharex=True, sharey=True, figsize=(20, 3*len(factors)))\n",
    "\n",
    "for j, factor in enumerate(factors):\n",
    "    handles.append(ax[j].bar(Xcopy3['date'], y_coefficents_big[:, j].squeeze(), label=factor , color=colors[j]) )\n",
    "    ax[j].set_ylabel(\"Factor Weight\")\n",
    "    plt.xticks(np.arange(0, len(Xcopy3['date']) + 1, 20))\n",
    "\n",
    "\n",
    "labels = [f for f in factors] +['unexplained']\n",
    "fig.suptitle(\"Wells Fargo 8 Factor Weights with Unexplained\", fontsize=30)\n",
    "\n",
    "plt.xlabel(\"Date\" , fontsize=20)\n",
    "lgd = plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-associate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-riverside",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-palmer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "first-boundary",
   "metadata": {},
   "source": [
    "Google asset Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scaledBig = datasetBig.copy()\n",
    "\n",
    "#only one of the following \n",
    "#dataset_scaledBig = dataset_scaledBig.drop(['google'], axis = 1)#for wfc results\n",
    "dataset_scaledBig = dataset_scaledBig.drop(['wfc'], axis = 1)#for google results\n",
    "\n",
    "train = dataset_scaledBig.iloc[:450,:]\n",
    "test = dataset_scaledBig.iloc[451:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For google results\n",
    "X_train = train.drop(['google'], axis = 1)\n",
    "y_train = train[[\"google\"]]\n",
    "\n",
    "X_test = test.drop(['google'], axis = 1)\n",
    "y_test = test[[\"google\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcopy2 = X_train\n",
    "Xcopy3 = X_test\n",
    "Xcopy2.reset_index(level=0, inplace=True)\n",
    "Xcopy3.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_intx = [dateutil.parser.parse(x) for x in list(Xcopy2['date'])]\n",
    "sorted_dates = sorted(dates_intx)\n",
    "sorted_dates = [x for x in sorted_dates ]\n",
    "sorted_dates_str = [str(x.date()) for x in sorted_dates]\n",
    "context = [c - sorted_dates[0] for c in sorted_dates[0:]]\n",
    "context = np.expand_dims(np.array([c.days for c in context]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_intx2 = [dateutil.parser.parse(x) for x in list(Xcopy3['date'])]\n",
    "sorted_dates2 = sorted(dates_intx2)\n",
    "sorted_dates2 = [x for x in sorted_dates2 ]\n",
    "sorted_dates_str2 = [str(x.date()) for x in sorted_dates2]\n",
    "context2 = [c - sorted_dates2[0] for c in sorted_dates2[0:]]\n",
    "context2 = np.expand_dims(np.array([c.days for c in context2]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_transformer = input_transformer(np.mean(context),\n",
    "                                           np.std(context),\n",
    "                                           append_ones=True)\n",
    "context_transformed = context_transformer.transform(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_transformer = input_transformer(np.mean(context2),\n",
    "                                           np.std(context2),\n",
    "                                           append_ones=True)\n",
    "context_transformed2 = context_transformer.transform(context2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_returns = X_train[['mtum','value', 'size' ]].values  #select columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y_train['google'].values\n",
    "\n",
    "asset_returns  = np.expand_dims(y, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context_transformed.shape, factor_returns.shape, asset_returns.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = ContextualizedRegressor(\n",
    "        context_shape=context_transformed.shape[1:],\n",
    "        data_shape=factor_returns.shape[1:],\n",
    "        output_shape=asset_returns.shape[1:],\n",
    "        n_archetypes=16,\n",
    "        sample_specific_loss_params=None,\n",
    "        archetype_loss_params={'regularizer': tf.keras.regularizers.l2(1e0)},\n",
    "        n_encoder_layers=5, encoder_width=32,\n",
    "        context_activity_regularizer=tf.keras.regularizers.l1(0.),\n",
    "        activation='sigmoid',\n",
    "        init_archs=None, freeze_archs=False,\n",
    "        learning_rate=1e-4,\n",
    "        tf_dtype=tf.dtypes.float32,\n",
    "        pop_model=None,\n",
    "        base_predictor=None,\n",
    "        encoder_type='dnn',\n",
    "        loss=\"mse\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model.fit(C=context_transformed,\n",
    "                 X=factor_returns, Y=asset_returns,\n",
    "                 epochs=100, batch_size=1,\n",
    "                 es_patience=3, val_split=0.25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-forest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_small = small_model.predict(context_transformed,factor_returns)\n",
    "\n",
    "r2_score(asset_returns, y_pred_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for few factors\n",
    "x_t=X_test[['mtum','value', 'size' ]].values\n",
    "x_t = np.expand_dims(x_t, axis = 1) \n",
    "x_t =np.squeeze(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-assumption",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t_small=y_test['google'].values\n",
    "\n",
    "y_t_small = np.expand_dims(y_t_small, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t_small = small_model.predict(context_transformed2, x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t_small = small_model.predict(context_transformed2, x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score( y_t_small, y_pred_t_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( y_t_small, y_pred_t_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-moses",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_returns = X_train[['mtum','value', 'size', 'bitcoin', 'cma', 'rwm', 'vix','rinf' ]].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y_train['google'].values\n",
    "\n",
    "asset_returns  = np.expand_dims(y, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_model = ContextualizedRegressor(\n",
    "        context_shape=context_transformed.shape[1:],\n",
    "        data_shape=factor_returns.shape[1:],\n",
    "        output_shape=asset_returns.shape[1:],\n",
    "        n_archetypes=16,\n",
    "        sample_specific_loss_params=None,\n",
    "        archetype_loss_params={'regularizer': tf.keras.regularizers.l2(1e0)},\n",
    "        n_encoder_layers=5, encoder_width=32,\n",
    "        context_activity_regularizer=tf.keras.regularizers.l1(0.),\n",
    "        activation='sigmoid',\n",
    "        init_archs=None, freeze_archs=False,\n",
    "        learning_rate=1e-4,\n",
    "        tf_dtype=tf.dtypes.float32,\n",
    "        pop_model=None,\n",
    "        base_predictor=None,\n",
    "        encoder_type='dnn',\n",
    "        loss=\"mse\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_model.fit(C=context_transformed,\n",
    "                 X=factor_returns, Y=asset_returns,\n",
    "                 epochs=100, batch_size=1,\n",
    "                 es_patience=3, val_split=0.25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_big = big_model.predict(context_transformed,factor_returns)\n",
    "\n",
    "r2_score(asset_returns, y_pred_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for big factors\n",
    "x_t=X_test[['mtum','value', 'size', 'bitcoin', 'cma', 'rwm', 'vix','rinf' ]].values  #select columns \n",
    "x_t = np.expand_dims(x_t, axis = 1) \n",
    "x_t =np.squeeze(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t_big=y_test['google'].values\n",
    "\n",
    "y_t_big = np.expand_dims(y_t_big, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_t_big = big_model.predict(context_transformed2, x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score( y_t_big, y_pred_t_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( y_t_big, y_pred_t_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do this for the predicted section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_coefficents = big_model.predict_coefs(context_transformed2)\n",
    "y_coefficents_small = big_model.predict_coefs(context_transformed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_coefficents_small.shape #row column , 82 inscances for the 8 factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-infrared",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is all for 3 factor at the moment \n",
    "#get with and without unexplained???\n",
    "\n",
    "#maybe lot the predicted and actual results too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-colonial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need the unexplaned information \n",
    "\n",
    "factors = ['mtum','value', 'size' ]\n",
    "for j, factor in enumerate(factors):\n",
    "    unexplained = (np.squeeze(y_t_small) - np.squeeze(y_pred_t_small)) / np.std(y_t_small)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible plot: unfinished. do this instead of using unexplained with coefficents??\n",
    "handles = []\n",
    "fig, ax = plt.subplots(3 ,1,sharex=True, sharey=True, figsize=(20, 3*len(factors)))\n",
    "handles.append(ax[0].bar(Xcopy3['date'], np.squeeze(y_pred_t_small), label='unexplaned' , color='red') )  #predicted\n",
    "handles.append(ax[1].bar(Xcopy3['date'], np.squeeze(y_t_small), label='unexplaned' , color='blue') )  #actual\n",
    "handles.append(ax[2].bar(Xcopy3['date'], unexplained.squeeze(), label='unexplaned' , color='slategrey') ) #unexplaned\n",
    "plt.xticks(np.arange(0, len(Xcopy3['date']) + 1, 20))\n",
    "\n",
    "fig.suptitle(\"Google 3 Factor Predicted, Actual, Unexplained\", fontsize=30)\n",
    "labels = ['Predicted', 'Actual', 'Unexplained']\n",
    "plt.xlabel(\"Date\" , fontsize=20)\n",
    "lgd = plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-provider",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-accordance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#its ploting strength of the factors, so postive and less then i 1 makes sense maybe?? missing something>>\n",
    "#incomplete results???\n",
    "factors = ['mtum','value', 'size' ]\n",
    "handles = []\n",
    "plot_offset=2\n",
    "colors=['red','blue', 'orange', 'black', 'purple', 'green', 'tan', 'cyan']\n",
    "\n",
    "fig, ax = plt.subplots(len(factors)+1 ,1,sharex=True, sharey=True, figsize=(20, 3*len(factors)))\n",
    "\n",
    "for j, factor in enumerate(factors):\n",
    "    handles.append(ax[j].bar(Xcopy3['date'], y_coefficents_small[:, j].squeeze(), label=factor , color=colors[j]) )\n",
    "    ax[j].set_ylabel(\"Factor Weight\")\n",
    "    plt.xticks(np.arange(0, len(Xcopy3['date']) + 1, 20))\n",
    "\n",
    "handles.append(ax[-1].bar(Xcopy3['date'], unexplained.squeeze(), label='unexplaned' , color='slategrey') )   \n",
    "ax[-1].set_ylabel(\"Factor Weight\")\n",
    "    \n",
    "labels = [f for f in factors] +['unexplained']\n",
    "fig.suptitle(\"Google 3 Factor Weights with Unexplained\", fontsize=30)\n",
    "\n",
    "plt.xlabel(\"Date\" , fontsize=20)\n",
    "lgd = plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-aurora",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = ['mtum','value', 'size' ]\n",
    "handles = []\n",
    "plot_offset=2\n",
    "colors=['red','blue', 'orange', 'black', 'purple', 'green', 'tan', 'cyan']\n",
    "\n",
    "fig, ax = plt.subplots(len(factors) ,1,sharex=True, sharey=True, figsize=(20, 3*len(factors)))\n",
    "\n",
    "for j, factor in enumerate(factors):\n",
    "    handles.append(ax[j].bar(Xcopy3['date'], y_coefficents_small[:, j].squeeze(), label=factor , color=colors[j]) )\n",
    "    ax[j].set_ylabel(\"Factor Weight\")\n",
    "    plt.xticks(np.arange(0, len(Xcopy3['date']) + 1, 20))\n",
    "\n",
    "\n",
    "    \n",
    "labels = [f for f in factors] \n",
    "fig.suptitle(\"Google 3 Factor Weights\", fontsize=30)\n",
    "\n",
    "plt.xlabel(\"Date\" , fontsize=20)\n",
    "lgd = plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-vancouver",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_coefficents_big = big_model.predict_coefs(context_transformed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "factors = ['mtum','value', 'size', 'bitcoin', 'cma', 'rwm', 'vix','rinf' ]\n",
    "for j, factor in enumerate(factors):\n",
    "    unexplained = (np.squeeze(y_t_big) - np.squeeze(y_pred_t_big)) / np.std(y_t_big)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible plot: unfinished. do this instead of using unexplained with coefficents??\n",
    "handles = []\n",
    "fig, ax = plt.subplots(3 ,1,sharex=True, sharey=True, figsize=(20, 3*3))\n",
    "handles.append(ax[0].bar(Xcopy3['date'], np.squeeze(y_pred_t_big), label='unexplaned' , color='red') )  #predicted\n",
    "handles.append(ax[1].bar(Xcopy3['date'], np.squeeze(y_t_big), label='unexplaned' , color='blue') )  #actual\n",
    "handles.append(ax[2].bar(Xcopy3['date'], unexplained.squeeze(), label='unexplaned' , color='slategrey') ) #unexplaned\n",
    "plt.xticks(np.arange(0, len(Xcopy3['date']) + 1, 20))\n",
    "\n",
    "fig.suptitle(\"Google 8 Factor Predicted, Actual, Unexplained\", fontsize=30)\n",
    "labels = ['Predicted', 'Actual', 'Unexplained']\n",
    "plt.xlabel(\"Date\" , fontsize=20)\n",
    "lgd = plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#its ploting strength of the factors, so postive and less then i 1 makes sense maybe?? missing something>>\n",
    "#incomplete results???\n",
    "factors = ['mtum','value', 'size', 'bitcoin', 'cma', 'rwm', 'vix','rinf' ]\n",
    "handles = []\n",
    "plot_offset=2\n",
    "colors=['red','blue', 'orange', 'black', 'purple', 'green', 'tan', 'skyblue']\n",
    "\n",
    "fig, ax = plt.subplots(len(factors) ,1,sharex=True, sharey=True, figsize=(20, 3*len(factors)))\n",
    "\n",
    "for j, factor in enumerate(factors):\n",
    "    handles.append(ax[j].bar(Xcopy3['date'], y_coefficents_big[:, j].squeeze(), label=factor , color=colors[j]) )\n",
    "    ax[j].set_ylabel(\"Factor Weight\")\n",
    "    plt.xticks(np.arange(0, len(Xcopy3['date']) + 1, 20))\n",
    "\n",
    "\n",
    "labels = [f for f in factors] +['unexplained']\n",
    "fig.suptitle(\"Google 8 Factor Weights \", fontsize=30)\n",
    "\n",
    "plt.xlabel(\"Date\" , fontsize=20)\n",
    "lgd = plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-process",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#its ploting strength of the factors, so postive and less then i 1 makes sense maybe?? missing something>>\n",
    "#incomplete results???\n",
    "factors = ['mtum','value', 'size', 'bitcoin', 'cma', 'rwm', 'vix','rinf' ]\n",
    "handles = []\n",
    "plot_offset=2\n",
    "colors=['red','blue', 'orange', 'black', 'purple', 'green', 'tan', 'skyblue']\n",
    "\n",
    "fig, ax = plt.subplots(len(factors) ,1,sharex=True, sharey=True, figsize=(20, 3*len(factors)))\n",
    "\n",
    "for j, factor in enumerate(factors):\n",
    "    handles.append(ax[j].bar(Xcopy3['date'], y_coefficents_big[:, j].squeeze(), label=factor , color=colors[j]) )\n",
    "    ax[j].set_ylabel(\"Factor Weight\")\n",
    "    plt.xticks(np.arange(0, len(Xcopy3['date']) + 1, 20))\n",
    "\n",
    "\n",
    "labels = [f for f in factors] +['unexplained']\n",
    "fig.suptitle(\"Google 8 Factor Weights \", fontsize=30)\n",
    "\n",
    "plt.xlabel(\"Date\" , fontsize=20)\n",
    "lgd = plt.legend(handles, labels, loc='upper left', bbox_to_anchor=(1, 0.6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-beauty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-chess",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
